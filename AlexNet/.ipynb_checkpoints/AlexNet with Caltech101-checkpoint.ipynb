{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:</br>\n",
    "https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40</br>\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils import data\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/jupyter_projects/PyTorch-Zoo/AlexNet\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Change this for your own directory.\n",
    "ROOT_DIR = Path(\"/root/jupyter_projects\")  # Your working directory.\n",
    "DATASET_DIR = ROOT_DIR / \"Data/Caltech101\"  # Folder to store dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/jupyter_projects\n",
      "/root/jupyter_projects/Data/Caltech101\n"
     ]
    }
   ],
   "source": [
    "print(ROOT_DIR)\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Define pytorch device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz to /root/jupyter_projects/Data/Caltech101/flowers-102/102flowers.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.3%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFlowers102\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mroot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDATASET_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Directory to store data.\u001B[39;49;00m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Download dataset from the internet.\u001B[39;49;00m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mToTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/flowers102.py:57\u001B[0m, in \u001B[0;36mFlowers102.__init__\u001B[0;34m(self, root, split, transform, target_transform, download)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_images_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_base_folder \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjpg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_integrity():\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/flowers102.py:107\u001B[0m, in \u001B[0;36mFlowers102.download\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_integrity():\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m \u001B[43mdownload_and_extract_archive\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_url_prefix\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_file_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_base_folder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmd5\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_file_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msetid\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    113\u001B[0m     filename, md5 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file_dict[\u001B[38;5;28mid\u001B[39m]\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/utils.py:430\u001B[0m, in \u001B[0;36mdownload_and_extract_archive\u001B[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m filename:\n\u001B[1;32m    428\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(url)\n\u001B[0;32m--> 430\u001B[0m \u001B[43mdownload_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_root\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmd5\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    432\u001B[0m archive \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(download_root, filename)\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExtracting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marchive\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mextract_root\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/utils.py:141\u001B[0m, in \u001B[0;36mdownload_url\u001B[0;34m(url, root, filename, md5, max_redirect_hops)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m url \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m to \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m fpath)\n\u001B[0;32m--> 141\u001B[0m     \u001B[43m_urlretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (urllib\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mURLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m url[:\u001B[38;5;241m5\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/utils.py:35\u001B[0m, in \u001B[0;36m_urlretrieve\u001B[0;34m(url, filename, chunk_size)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39murlopen(urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39mRequest(url, headers\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser-Agent\u001B[39m\u001B[38;5;124m\"\u001B[39m: USER_AGENT})) \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39mresponse\u001B[38;5;241m.\u001B[39mlength) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m---> 35\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m: response\u001B[38;5;241m.\u001B[39mread(chunk_size), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     36\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m chunk:\n\u001B[1;32m     37\u001B[0m                 \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/utils.py:35\u001B[0m, in \u001B[0;36m_urlretrieve.<locals>.<lambda>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39murlopen(urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39mRequest(url, headers\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser-Agent\u001B[39m\u001B[38;5;124m\"\u001B[39m: USER_AGENT})) \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39mresponse\u001B[38;5;241m.\u001B[39mlength) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m---> 35\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     36\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m chunk:\n\u001B[1;32m     37\u001B[0m                 \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.8/http/client.py:455\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;66;03m# Amount is given, implement using readinto\u001B[39;00m\n\u001B[1;32m    454\u001B[0m     b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(amt)\n\u001B[0;32m--> 455\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b)[:n]\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001B[39;00m\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;66;03m# and self.chunked\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.8/http/client.py:499\u001B[0m, in \u001B[0;36mHTTPResponse.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    494\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmemoryview\u001B[39m(b)[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength]\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001B[39;00m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001B[39;00m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# (for example, reading in 1k chunks)\u001B[39;00m\n\u001B[0;32m--> 499\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m n \u001B[38;5;129;01mand\u001B[39;00m b:\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    502\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    503\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m/usr/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.8/ssl.py:1241\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1238\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1239\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1240\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/usr/lib/python3.8/ssl.py:1099\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_data = datasets.Flowers102(\n",
    "    root=DATASET_DIR,  # Directory to store data.\n",
    "    download=True,  # Download dataset from the internet.\n",
    "    transform=transforms.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_DIM = 227  # Image dimension: (3, 227, 227)\n",
    "NUM_CLASSES = 101\n",
    "\n",
    "data_path = DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/root/jupyter_projects/Data/Caltech101\n"
    }
   ],
   "source": "print(data_path)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "102\n"
    }
   ],
   "source": "print(len(next(os.walk(data_path))[1]))"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "img_transforms = transforms.Compose([\n    transforms.CenterCrop(IMAGE_DIM),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225]),\n])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Image for each class is stored in a separate folder, we can use `datasets.ImageFolder` to create dataset."
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mImageFolder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mroot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg_transforms\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal number of samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:310\u001B[0m, in \u001B[0;36mImageFolder.__init__\u001B[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    304\u001B[0m     root: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    308\u001B[0m     is_valid_file: Optional[Callable[[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m ):\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mIMG_EXTENSIONS\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_valid_file\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_transform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_transform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_valid_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_valid_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:146\u001B[0m, in \u001B[0;36mDatasetFolder.__init__\u001B[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(root, transform\u001B[38;5;241m=\u001B[39mtransform, target_transform\u001B[38;5;241m=\u001B[39mtarget_transform)\n\u001B[1;32m    145\u001B[0m classes, class_to_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfind_classes(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot)\n\u001B[0;32m--> 146\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_to_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextensions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_valid_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader \u001B[38;5;241m=\u001B[39m loader\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mextensions \u001B[38;5;241m=\u001B[39m extensions\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:190\u001B[0m, in \u001B[0;36mDatasetFolder.make_dataset\u001B[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m class_to_idx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001B[39;00m\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001B[39;00m\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001B[39;00m\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe class_to_idx parameter cannot be None.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 190\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmake_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_to_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextensions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_valid_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_valid_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:103\u001B[0m, in \u001B[0;36mmake_dataset\u001B[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m extensions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    102\u001B[0m         msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSupported extensions are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mextensions \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(extensions, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(extensions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(msg)\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m instances\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    }
   ],
   "source": "dataset = datasets.ImageFolder(\n    root=data_path, \n    transform=img_transforms\n)\nprint(f\"Total number of samples: {len(dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "# Split the data into training set and validation set.\ntrain_data_size = int(len(dataset) * 0.8)\nval_data_size = len(dataset) - train_data_size\ntrain_data, val_data = data.random_split(dataset, [train_data_size, val_data_size])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "print(f\"Total number of samples in training set: {len(train_data)}\")\nprint(f\"Total number of samples in validation set: {len(val_data)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "train_dataloader = data.DataLoader(train_data, batch_size=BATCH_SIZE)\nval_dataloader = data.DataLoader(val_data, batch_size=BATCH_SIZE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "print(f\"Number of iterations in training set: {len(train_dataloader)}\")\nprint(f\"Number of iterations in validation set: {len(val_dataloader)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "trusted": false
   },
   "outputs": [],
   "source": "for batch, (X, y) in enumerate(train_dataloader):\n    print(batch)\n    print(X.shape)\n    \n    plt.imshow(X[17].permute(1, 2, 0))\n    plt.show()\n    \n    print(y.shape)\n    print(y[17])\n    break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "class AlexNet(nn.Module):\n    def __init__(self, num_classes: int = NUM_CLASSES, dropout: float = 0.5) -> None:\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Linear(256 * 6 * 6, 4096),  # !\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=dropout),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "alexnet = AlexNet()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "summary(alexnet, (3, 227, 227))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "seed = torch.initial_seed()\nprint('*Used seed : {}*'.format(seed))  # seed value"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "optimizer = optim.Adam(params=alexnet.parameters(), lr=0.0001)\nprint('Optimizer created')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "loss_fn = nn.CrossEntropyLoss()\nprint('Lost function created')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "def train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)  # Total number of samples in training set.\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):  # Iterations.\n        X, y = X.to(device), y.to(device)\n        \n        # Compute prediction error.\n        y_pred = model(X)\n        loss = loss_fn(y_pred, y)\n        \n        # Backpropagation.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)  # Iteration * Batch Size = Number of samples have being trained.\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "def test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)  # Total number of samples in test set.\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            test_loss += loss_fn(y_pred, y).item()\n            correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": "epochs = 5\nmodel = alexnet\n\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(val_dataloader, model, loss_fn)\nprint(\"Done!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}